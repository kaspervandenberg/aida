/*
 * LingPipe v. 1.0
 * Copyright (C) 2003 Alias-i
 *
 * This program is licensed under the Alias-i Royalty Free License
 * Version 1 WITHOUT ANY WARRANTY, without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Alias-i
 * Royalty Free License Version 1 for more details.
 *
 * You should have received a copy of the Alias-i Royalty Free License
 * Version 1 along with this program; if not, visit
 * http://www.alias-i.com/lingpipe/licenseV1.txt or contact
 * Alias-i, Inc. at 181 North 11th Street, Suite 401, Brooklyn, NY 11211,
 * +1 (718) 290-9170.
 */

package com.aliasi.lm;

import com.aliasi.util.AbstractWritable;
import com.aliasi.util.Arrays;
import com.aliasi.util.ObjectToCounter;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.Strings;

import java.io.DataOutput;
import java.io.IOException;

import java.text.DecimalFormat;

import java.util.LinkedList;

/**
 * An <code>NGramProcessLM</code> provides a dynamic conditional
 * process language model process for which training, estimation, and
 * pruning may be interleaved.  A process language model normalizes
 * probablities for a given length of input. The model may be written
 * to an output stream in binary form at any time and a static method
 * is provided to read a language model process from this binary
 * format.  The resulting model read back in is no longer dynamic, but
 * will be much faster.
 *
 * <P>This class implements a generative language model based on the
 * chain rule (see the class documentation for {@link
 * LanguageModel.Conditional} for details).  class The n-gram
 * approximation reduces the context to the previous <code>n-1</code>
 * characters:
 *
 * <blockquote><code>
 *   P(c<sub><sub>k</sub></sub>|c<sub><sub>0</sub></sub>,c<sub><sub>1</sub></sub>,...,c<sub><sub>k-1</sub></sub>)
 *   ~ P(c<sub><sub>k</sub></sub>|c<sub><sub>k-n+1</sub></sub>,...,c<sub><sub>k-1</sub></sub>)
 * </code></blockquote>
 *
 * Estimates are smoothed using linear interpolation of the maximum
 * likelihood estimate <code>P<sub><sub>ML</sub></sub></code> at the full context
 * with the estimate for the next shorter context.  The maximum likelihood
 * estimate is just the frequency-based estimate, as described in the
 * class documentation for {@link CharSeqCounter}.
 *
 * <P>Smoothing is carried out with linear interpolation of contexts
 * with the next lower order context.  The interpolation ratio is
 * derived using a parametric variant of Witten-Bell method
 * C (see below for references):
 *
 * <blockquote><code>
 *   P(c<sub><sub>k</sub></sub>|c<sub><sub>j</sub></sub>,...,c<sub><sub>k-1</sub></sub>)
 *   <br>
 *   = lambda(c<sub><sub>j</sub></sub>,...,c<sub><sub>k-1</sub></sub>)
 *     * P<sub><sub>ML</sub></sub>(c<sub><sub>k</sub></sub>|c<sub><sub>j</sub></sub>,...,c<sub><sub>k-1</sub></sub>)
 *   <br> &nbsp; &nbsp;
 *   + (1-lambda(c<sub><sub>j</sub></sub>,...,c<sub><sub>k-1</sub></sub>))
 *     * P(c<sub><sub>k</sub></sub>|c<sub><sub>j</sub></sub>,...,c<sub><sub>k-2</sub></sub>)
 * </code></blockquote>
 *
 * This class uses a parametric variant of Witten-Bell method C, which
 * was introduced in:
 *
 * In the parametric Witten-Bell method, the interpolation ratio
 * <code>lambda</code> is defined based on the context of estimation
 * to be:
 *
 * <blockquote><code>
 *   lambda(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>)
 *   <br> = freq(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>)
 *   <br> &nbsp; &nbsp; / (freq(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>)
 *                         + L * numExtensions(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>))
 * </code></blockquote>
 *
 * where
 * <code>c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub></code>
 * is the conditioning context for estimation,
 * <code>numExtensions(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>)</code>
 * is the number of distinct outcomes seen in the context, and
 * <code>L</code> is a hyper-parameter of the distribution.  As a base
 * case, <code>P(c<sub><sub>k</sub></sub>)</code> is interpolated with
 * the uniform distribution in the same manner:
 *
 * <blockquote><code>
 *   P(c<sub><sub>k</sub></sub>)
 *   ~ lambda() * P<sub><sub>ML</sub></sub>(c<sub><sub>k</sub></sub>) + (1-lambda()) * P<sub><sub>U</sub></sub>(c<sub><sub>k</sub></sub>)
 * </code></blockquote>
 *
 * where <code>lambda()</code> is defined based on counts for the null
 * context, or in other words, on the individual character outcomes.
 * The uniform distribution
 * <code>P<sub><sub>U</sub></sub></code> only depends on the number of
 * possible characters used; specifically,
 * <code>P<sub><sub>U</sub></sub>(c) = 1/alphabetSize</code> where
 * <code>alphabetSize</code> is the maximum number of distinct
 * characters in this model.
 *
 * <P>The free hyperparameter <code>L</code> in the smoothing equation
 * determines the balance between higher-order and lower-order models.
 * A higher value for <code>L</code> gives more of the weight to
 * lower-order contexts.  As the amount of data grows against a fixed
 * alphabet of characters, the impact of <code>L</code> is reduced.
 * In Witten and Bell's original paper, the hyperparameter
 * <code>L</code> was set to 1.0, which is not a particularly good
 * choice for most text sources.  A value of the lambda factor that is
 * roughly the length of the longest n-gram seems to be a good rule of
 * thumb.
 *
 * <P>An n-gram language model process is constructed by specifying
 * three things: the maximum length of n-gram to use, the
 * hyperparameter for interpolation of higher-order and lower-order
 * estimates, and the maximum number of characters to be seen in
 * training and estimation.
 *
 * <P>Methods are provided for computing a sample cross-entropy rate
 * for a character sequence.  The sample cross-entropy
 * <code>H(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>;P<sub><sub>M</sub></sub>)</code> for
 * sequence <code>c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub></code> in
 * probability model <code>P<sub><sub>M</sub></sub></code> is defined to be the
 * average log (base 2) probability of the characters in the sequence
 * according to the model. In symbols:
 *
 * <blockquote><code>
 * H(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>;P<sub><sub>M</sub></sub>)
=  (-log<sub><sub>2</sub></sub> P<sub><sub>M</sub></sub>(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>))/n
 * </code></blockquote>
 *
 * The cross-entropy rate of distribution <code>P'</code>
 * with respect to a distribution <code>P</code> is defined by:
 *
 * <blockquote><code>
 *   H(P',P)
 *   = <big><big><big>&Sigma;</big></big></big><sub><sub>x</sub></sub>
 *     P(x) * log<sub><sub>2</sub></sub> P'(x)
 * </code></blockquote>
 *
 * The Shannon-McMillan-Breiman theorem shows that as the length of
 * the sample drawn from the true distribution <code>P</code> grows,
 * the sample cross-entropy rate approaches the actual cross-entropy
 * rate.  In symbols:
 *
 * <blockquote>
 * <code>
 *  H(P,P<sub><sub>M</sub></sub>)
 *  = lim<sub><sub>n->infinity</sub></sub>
 *    H(c<sub><sub>1</sub></sub>,...,c<sub><sub>n</sub></sub>;P<sub><sub>M</sub></sub>)/n
 * </code>
 * </blockquote>
 *
 * The entropy of a distribution <code>P</code> is defined by its
 * cross-entropy against itself, <code>H(P,P)</code>.  A
 * distribution's entropy is a lower bound on its cross-entropy; in
 * symbols, <code>H(P',P) > H(P,P)</code> for all distributions
 * <code>P'</code>.
 *
 * <P><i>References</i>
 * <blockquote>
 * Witten, Ian H. and Timothy C. Bell.  1991. The zero-frequency
 * problem: estimating the probabilities of novel events in adaptive
 * text compression. <i>IEEE Transactions on Information Theory</i>
 * <b>37</b>(4).
 * </blockquote>
 *
 * @author Bob Carpenter
 * @version 2.0
 * @since   LingPipe2.0
 */
public class NGramProcessLM
    extends AbstractWritable
    implements LanguageModel.Process,
               LanguageModel.Conditional,
               LanguageModel.Dynamic {

    private final TrieCharSeqCounter mTrieCharSeqCounter;

    private final int mNumChars;
    private final int mMaxNGram;
    private final double mUniformEstimate;
    private final double mLog2UniformEstimate;
    private final double mLambdaFactor;

    /**
     * Constructs an n-gram process language model with default
     * maximum length n-gram (8), default number of characters ({@link
     * Character#MAX_VALUE}), and default hyperparameter (equal to
     * n-gram length).
     */
    public NGramProcessLM() {
        this(8,Character.MAX_VALUE);
    }

    /**
     * Construct an n-gram language process with the specified maximum
     * n-gram length and maximum number of characters.  The interpolation
     * hyperparameter will be set to the same value as the maximum n-gram
     * length.
     *
     * @param maxNGram Maximum length n-gram for which counts are
     * stored.
     * @param numChars Maximum number of characters in training data.
     */
    public NGramProcessLM(int maxNGram,
                          int numChars) {
        this(maxNGram,numChars,maxNGram);
    }

    /**
     * Construct an n-gram language process with the specified maximum
     * n-gram length, number of characters, starting lambda factor,
     * along with sizes for rolling averages and period of sampling in
     * training data.
     *
     * @param maxNGram Maximum length n-gram for which counts are
     * stored.
     * @param numChars Maximum number of characters in training data.
     * @param lambdaFactor Central value of interpolation
     * hyperparameter explored.
     */
    public NGramProcessLM(int maxNGram,
                          int numChars,
                          double lambdaFactor) {
        mMaxNGram = maxNGram;
        mTrieCharSeqCounter = new TrieCharSeqCounter(maxNGram);
        mNumChars = numChars;
        mLambdaFactor = lambdaFactor;
        mUniformEstimate = 1.0 / (double)mNumChars;
        mLog2UniformEstimate
            = com.aliasi.util.Math.log2(mUniformEstimate);
    }

    public String readClassName() {
        return CompiledNGramProcessLM.class.getName();
    }

    public final double log2Estimate(CharSequence cSeq) {
        char[] cs = Strings.toCharArray(cSeq);
        return log2Estimate(cs,0,cs.length);
    }

    public final double log2Estimate(char[] cs, int start, int end) {
        Strings.checkArgsStartEnd(cs,start,end);
        double sum = 0.0;
        for (int i = start+1; i <= end; ++i)
            sum += log2ConditionalEstimate(cs,start,i);
        return sum;
    }

    /**
     * Returns the substring counter for this language model.
     * Modifying the counts in the returned counter, such as by
     * pruning, will change the estimates in this language model.
     *
     * @return Substring counter for this language model.
     */
    public TrieCharSeqCounter substringCounter() {
        return mTrieCharSeqCounter;
    }

    public void train(CharSequence cSeq) {
        char[] cs = Strings.toCharArray(cSeq);
        train(cs,0,cs.length);
    }

    public void train(char[] cs, int start, int end) {
        Strings.checkArgsStartEnd(cs,start,end);
        mTrieCharSeqCounter.incrementSubstrings(cs,start,end);
    }


    public void writeData(DataOutput dataOut) throws IOException {
        dataOut.writeInt(mMaxNGram);

        float logUniformEstimate
            = (float) mLog2UniformEstimate;
        dataOut.writeFloat(logUniformEstimate);

        long numNodes = mTrieCharSeqCounter.uniqueSequenceCount();
        if (numNodes > Integer.MAX_VALUE) {
            String msg = "Maximum number of compiled nodes is"
                + " Integer.MAX_VALUE = " + Integer.MAX_VALUE
                + " Found number of nodes=" + numNodes;
            throw new IllegalArgumentException(msg);
        }
        dataOut.writeInt((int)numNodes);

        int lastInternalNodeIndex = lastInternalNodeIndex();
        dataOut.writeInt((int)lastInternalNodeIndex);

        LinkedList queue = new LinkedList();
        // write root node (char,logP,log(1-L),firstDtr)
        dataOut.writeChar('\uFFFF');
        dataOut.writeFloat(logUniformEstimate);
        double oneMinusLambda = 1.0 - lambda(mTrieCharSeqCounter.mRootNode);
        float log2OneMinusLambda
            = (float) com.aliasi.util.Math.log2(oneMinusLambda);
        dataOut.writeFloat(log2OneMinusLambda);
        dataOut.writeInt(1);  // firstDtr
        char[] cs = mTrieCharSeqCounter.observedCharacters();
        for (int i = 0; i < cs.length; ++i)
            queue.add(new char[] { cs[i] });
        for (int index = 1; !queue.isEmpty(); ++index) {
            char[] nGram = (char[]) queue.removeFirst();
            char c = nGram[nGram.length-1];
            dataOut.writeChar(c);

            float logConditionalEstimate
                = (float) log2ConditionalEstimate(nGram,0,nGram.length);
            dataOut.writeFloat(logConditionalEstimate);

            if (index <= lastInternalNodeIndex) {
                double oneMinusLambda2 = 1.0 - lambda(nGram,0,nGram.length);
                float log2OneMinusLambda2
                    = (float) com.aliasi.util.Math.log2(oneMinusLambda2);
                dataOut.writeFloat(log2OneMinusLambda2);
                int firstChildIndex = index + queue.size() + 1;
                dataOut.writeInt(firstChildIndex);
            }
            char[] cs2
                = mTrieCharSeqCounter.charactersFollowing(nGram,0,nGram.length);
            for (int i = 0; i < cs2.length; ++i)
                queue.add(com.aliasi.util.Arrays.concatenate(nGram,cs2[i]));
        }
    }

    private void checkValidNGram(int nGramLength) {
        if (nGramLength > mMaxNGram) {
            String msg = "NGram beyond maximum ngram."
                + " Maximum n-gram=" + mMaxNGram
                + " Argument n=" + nGramLength;
            throw new IllegalArgumentException(msg);
        }
    }

    /**
     * Returns a count of the total number of characters handled
     * for training.  Note that this count is before any pruning
     * took place.  The underlying character sequence counter also
     * holds counts for this model.
     *
     * @return A count of the total number of characters handled
     * for training.
     */
    public long trainingEventCount() {
        return mNumTrainingEvents;
    }

    private static final boolean UPDATE_EXCLUSION = false;

    private static final String WITTEN_BELL = "WB";
    private static final String DIRICHLET = "Di";
    private static final String ABSOLUTE_DISCOUNTING = "AD";
    private static final String SMOOTHING_TYPE = WITTEN_BELL;


    String reportTopNGrams(int nGramOrder, int maxReturned) {
        return mTrieCharSeqCounter.topNGrams(nGramOrder,maxReturned).toString();
    }

    String reportContextHeader() {
        // header
        StringBuffer sb = new StringBuffer();
        sb.append("Training Data,Corpus");
        for (int i = 1; i <= mMaxNGram; ++i) {
            sb.append(',');
            sb.append("AVG_");
            sb.append(i);
            sb.append(',');
            sb.append("UNIQ_");
            sb.append(i);
        }
        sb.append('\n');
        return sb.toString();
    }

    // reports log2 count x log2 rank; results are only 1
    String reportContext(String corpusName) {
        StringBuffer sb = new StringBuffer();

        // row per training data size
        sb.append(corpusName);
        sb.append(',');
        sb.append(mNumTrainingEvents);
        for (int i = 1; i <= mMaxNGram; ++i) {
            sb.append(',');
            long totalCount = mTrieCharSeqCounter.totalSequenceCount(i);
            long uniqueCount = mTrieCharSeqCounter.uniqueSequenceCount(i);
            double logTotalCount = com.aliasi.util.Math.log2(totalCount);
            double logUniqueCount = com.aliasi.util.Math.log2(uniqueCount);
            double logAvgCount = logTotalCount - logUniqueCount;
            sb.append(decimalFormat(logAvgCount));
            sb.append(',');
            sb.append(decimalFormat(logUniqueCount));
        }
        sb.append('\n');
        return sb.toString();
    }

    /*
        int[] counts = mTrieCharSeqCounter.nGramFrequencies(nGramOrder);
        sb.append("N-Gram Order=" + nGramOrder + '\n');
        sb.append("Num Unique " + nGramOrder + "-grams=" + counts.length + '\n');
        int count = mTrieCharSeqCounter.totalNGramCount(nGramOrder);
        sb.append("Total Count of " + nGramOrder + "-grams=" + count + '\n');
        sb.append("Average count per " + nGramOrder + "-gram=" + ((double)count)/(double)counts.length + '\n');
        sb.append("RANK,COUNT,log2(COUNT)\n");
        for (int i = 1; i < counts.length; i *= 2) {
            if (i > 1) sb.append("\n");
            sb.append(i + "," + counts[i-1] + "," + com.aliasi.util.Math.log2((double)counts[i-1]));
        }
        return sb.toString();
    */


    String reportSmoothing() {
        return UPDATE_EXCLUSION
            ? SMOOTHING_TYPE + "-UE"
            : SMOOTHING_TYPE;
    }

    String reportEntropy(String corpus) {
        StringBuffer sb = new StringBuffer();
        // CORPUS, SMOOTHING, TRAINING-CHARS, bestAt1,bestAt2,...,bestAtN
        sb.append(corpus);
        sb.append(',');
        sb.append(reportSmoothing());
        sb.append(',');
        sb.append(mNumTrainingEvents);
        for (int n = 1; n <= mMaxNGram; ++n) {
            double bestEntropyRate = sampleEntropyRate(n,1);
            int bestIndex = 1;
            for (int i = 2; i < mNumLambdas; ++i) {
                double sampleRate = sampleEntropyRate(n,i);
                if (sampleRate < bestEntropyRate) {
                    bestIndex = i;
                    bestEntropyRate = sampleRate;
                }
            }
            sb.append(',');
            sb.append(decimalFormat(mLambdaFactors[bestIndex]));
            sb.append(',');
            sb.append(decimalFormat(bestEntropyRate));
            sb.append(',');
            sb.append(decimalFormat(sampleVariance(n,bestIndex)));
        }
        sb.append('\n');
        return sb.toString();
    }

    private static final DecimalFormat DEC_FORMAT
        = new DecimalFormat("#.000");
    private static String decimalFormat(double x) {
        // return Double.toString(x);
        return DEC_FORMAT.format(x);
    }

    String reportContexts() {
        StringBuffer sb = new StringBuffer();
        double total = 0.0;
        for (int k = 0; k < mContextUsedHistogram.length; ++k)
            total += mContextUsedHistogram[k];
        for (int k = 0; k < mContextUsedHistogram.length; ++k) {
            sb.append("    ");
            sb.append(k == 0 ? "U" : Integer.toString(k-1));
            sb.append(" ");
            sb.append(mContextUsedHistogram[k]);
            sb.append(" ");
            sb.append(100.0*mContextUsedHistogram[k]/total);
            sb.append("%");
            sb.append('\n');
        }
        return sb.toString();
    }

    String reportNodeTypes() {
        return nodeTypeCount().toString();
    }

    public double log2ConditionalEstimate(CharSequence cSeq) {
        return log2ConditionalEstimate(cSeq,mMaxNGram);
    }

    double log2ConditionalEstimate(CharSequence cSeq, int maxNGram) {
        return log2ConditionalEstimate(Arrays.toArray(cSeq),
                                       0,cSeq.length(),maxNGram);
    }

    public double log2ConditionalEstimate(char[] cs, int start, int end) {
        Strings.checkArgsStartEnd(cs,start,end);
        return log2ConditionalEstimate(cs,start,end,mMaxNGram);
    }

    double log2ConditionalEstimate(char[] cs, int start, int end,
                                   int maxNGram) {
        return
            -log2ConditionalEstimatess(cs,start,end)
            [mNumLambdaMultipliers]
            [maxNGram];
    }

    /**
     * Returns a string-based representation of this language model.
     * Note that this string may be very large, as it contains a
     * string representation of the counts in the trie structure
     * underlying this model.
     *
     * @return String-based representation of this model.
     */
    public String toString() {
        return mTrieCharSeqCounter.mRootNode.toString();
    }

    double[][] log2ConditionalEstimatess(char[] cs, int start, int end) {
        double[][] negLogEstimates = new double[mNumLambdas][mMaxNGram+1];
        if (start == end) {
            for (int i = 0; i < mNumLambdas; ++i)
                for (int j = 0; j < negLogEstimates.length; ++j)
                    negLogEstimates[i][j] = -mLog2UniformEstimate;
            return negLogEstimates;
        }
        double[] currentEstimate = new double[mNumLambdas];
        for (int i = 0; i < mNumLambdas; ++i) {
            negLogEstimates[i][0] = -mLog2UniformEstimate;
            currentEstimate[i] = mUniformEstimate;
        }
        int contextEnd = end-1;
        int longestContextStart = Math.max(start,end-mMaxNGram);
        int maxContextUsed = 0;
        for (int currentContextStart = contextEnd;
             currentContextStart >= longestContextStart;
             --currentContextStart) {
            long contextCount
                = mTrieCharSeqCounter.extensionCount(cs,currentContextStart,contextEnd);
            if (contextCount == 0) break;
            ++maxContextUsed;
            long outcomeCount = mTrieCharSeqCounter.count(cs,currentContextStart,end);
            if (UPDATE_EXCLUSION && currentContextStart > longestContextStart) {
                long[] ueCounts = updateExclusionCounts(cs,currentContextStart,end);
                if (ueCounts[1] == 0) break;
                outcomeCount = ueCounts[0];
                contextCount = ueCounts[1];
            }
            for (int i = 0; i < mNumLambdas; ++i) {
                if (SMOOTHING_TYPE.equals(ABSOLUTE_DISCOUNTING)) {
                    // absolute discounting  (0 <= mLambdaFactors[i] < 1.0)
                    double numOutcomes
                        = mTrieCharSeqCounter.numCharactersFollowing(cs,currentContextStart,contextEnd);
                    double smooth
                        = numOutcomes * mLambdaFactors[i] / (double)contextCount;
                    double discountedOutcomeCount
                        = ((double)outcomeCount) - mLambdaFactors[i];
                    if (discountedOutcomeCount < 0.0) discountedOutcomeCount = 0.0;
                    currentEstimate[i]
                        = discountedOutcomeCount/((double)contextCount)
                        + smooth * currentEstimate[i];
                } else if (SMOOTHING_TYPE.equals(DIRICHLET)) {
                    // Dirichlet Smoothing (aka Bayesian smoothing)
                    currentEstimate[i]
                        = (outcomeCount + mLambdaFactors[i] * currentEstimate[i])
                        / (contextCount + mLambdaFactors[i]);
                } else if (SMOOTHING_TYPE.equals(WITTEN_BELL)) {
                    // Linear Interpolation  (0 < mLambdaFactors[i])
                    double lambda = lambda(cs,currentContextStart,contextEnd,i);
                    if (lambda < 0.0 || lambda > 1.0)
                        throw new IllegalStateException("bad lambda=" + lambda);
                    currentEstimate[i]
                        = lambda * (((double)outcomeCount) / (double)contextCount)
                        + (1.0 - lambda) * currentEstimate[i];
                }
                negLogEstimates[i][contextEnd-currentContextStart+1]
                    = -com.aliasi.util.Math.log2(currentEstimate[i]);
            }
        }
        ++mContextUsedHistogram[maxContextUsed];
        for (int i = 0; i < mNumLambdas; ++i)
            for (int j = maxContextUsed + 1;
                 j < negLogEstimates[i].length;
                 ++j)
                negLogEstimates[i][j] = negLogEstimates[i][j-1];
        return negLogEstimates;
    }

    private void updateAverageEntropy(char[] cs, int start, int end) {
        ++mNumRollingAverageEvents;
        if (mNumRollingAverageEvents % mRollingAverageSamplePeriod != 0) return;
        computeAverageEntropyChar(cs,start,end);
    }

    private void computeAverageEntropyChar(char[] cs, int start, int end) {
        double[][] negLogEstimates = log2ConditionalEstimatess(cs,start,end);
        mRollingAverageNextIndex++;
        if (mRollingAverageNextIndex >= mRollingAverageSize)
            mRollingAverageNextIndex = 0;
        for (int i = 0; i < mNumLambdas; ++i)
            for (int n = 1; n <= mMaxNGram; ++n)
                mRollingAverageSamples[i][n][mRollingAverageNextIndex]
                    = (float) negLogEstimates[i][n];
    }

    double lambda(char[] cs, int start, int end) {
        Strings.checkArgsStartEnd(cs,start,end);
        return lambda(cs,start,end,mNumLambdaMultipliers);
    }

    double lambda(char[] cs, int start, int end, int lambdaIndex) {
        // Christer's recommended scaling to
        // double count = Math.sqrt(mTrieCharSeqCounter.extensionCount(cs,start,end));
        double count = mTrieCharSeqCounter.extensionCount(cs,start,end);
        if (count <= 0.0) return 0.0;
        double numOutcomes = mTrieCharSeqCounter.numCharactersFollowing(cs,start,end);
        return lambda(count,numOutcomes,lambdaIndex);
    }
    double lambda(double count, double numOutcomes, int lambdaIndex) {
        return count
            / (count + mLambdaFactors[lambdaIndex] * numOutcomes);
    }
    double lambda(Node node) {
        return lambda(node,mNumLambdaMultipliers);
    }
    double lambda(Node node, int lambdaIndex) {
        double count = node.contextCount(CS_DUMMY,0,0);
        double numOutcomes = node.numOutcomes(CS_DUMMY,0,0);
        return lambda(count,numOutcomes,lambdaIndex);
    }

    private static final char[] CS_DUMMY = new char[0];

    private int lastInternalNodeIndex() {
        int last = 1;
        LinkedList queue = new LinkedList();
        queue.add(mTrieCharSeqCounter.mRootNode);
        for (int i = 1; !queue.isEmpty(); ++i) {
            Node node = (Node) queue.removeFirst();
            if (node.numOutcomes(com.aliasi.util.Arrays.EMPTY_CHAR_ARRAY,
                                 0,0) > 0)
                last = i;
            node.addDaughters(queue);
        }
        return last-1;
    }

    void decrementUnigram(char c) {
        mTrieCharSeqCounter.decrementUnigram(c);
    }

    private long[] updateExclusionCounts(char[] cs, int start, int end) {
        long numerator = 0;
        long denominator = 0;
        char outcomeChar = cs[end-1];
        int length = end-start;
        char[] csEstimate = new char[length+1];
        for (int i = 0; i < length; ++i)
            csEstimate[i+1] = cs[start+i];  // leave first open
        char[] allChars = mTrieCharSeqCounter.observedCharacters();
        for (int i = 0; i < allChars.length; ++i) {
            csEstimate[0] = allChars[i];
            int numOutcomes = mTrieCharSeqCounter.numCharactersFollowing(csEstimate,0,length);
            if (numOutcomes == 0) continue;
            denominator += numOutcomes;
            if (mTrieCharSeqCounter.count(csEstimate,0,length+1) > 0)
                ++numerator;
        }
        return new long[] { numerator, denominator };
    }

    static double mean(float[] xs) {
        double sum = 0.0;
        for (int i = 0; i < xs.length; ++i)
            sum += xs[i];
        return sum / (double) xs.length;
    }

    static double variance(float[] xs) {
        double mean = mean(xs);
        double sumOfSquareDiffs = 0.0;
        for (int i = 0; i < xs.length; ++i) {
            double diff = xs[i] - mean;
            sumOfSquareDiffs += diff * diff;
        }
        return sumOfSquareDiffs / (double) xs.length;
    }



}

